---
title:      "XOR Neural Network"
---


We are going to build a simple neural network from scratch to recognize XOR function.
This is the "hello world" equivalent of machine learning, and we'll build it using TensorFlow

Here is what we want to be able to learn, giving 2 vectors A and B :

|   A  |   B  | A XOR B |
|:----:|:----:|:-------:|
| 0    | 0    |    0    |
| 0    | 1    |    1    |
| 1    | 0    |    1    |
| 1    | 1    |    0    |

Since we know the real outputs of the function, we say that we do <b>supervised learning</b>

## Neural Net model

Input vector is all the combinations of 2 bits ([0,0];[0,1];[1,0];[1,1]).
Output vector is a number between 0 and 1 that represents the probability of being a 0 or 1.
Here is the neural net :

![XOR neural net model]({{ site.baseurl }}/img/2016-12-15-xor-nn/xor_nn_model.png)

## Code

### Activation functions

We do a logistic regression so we'll use a classical linear function : y = ax + b.

Now have a look at 2 functions : tanh and sigmoid that transform our output values to probabilities.

### tanh

![Hyperbolic Tangent](https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/1200px-Hyperbolic_Tangent.svg.png)

{% highlight python linenos %}
  import numpy as np

  def tanh(x):
      return np.tanh(x)

  def tanh_p(x):
      return 1.0 - x**2
{% endhighlight %}

### sigmoid

![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/9/9d/Sigmoide.PNG)

{% highlight python linenos %}
  def sigmoid(x):
      return 1.0/(1.0 + np.exp(-x))

  def sigmoid_p(x):
      return sigmoid(x)*(1.0-sigmoid(x))
{% endhighlight %}

We can see see that these 2 functions are similarly good to discriminate values to -1 and +1.

## TensorFlow

### create the model

Placeholders are memory emplacement which will be feed during the program.
Variables will be define during the TF session.
We also use the built-in sigmoid function of TensorFlow.

{% highlight python linenos %}
  import tensorflow as tf

  x_ = tf.placeholder(tf.float32, [4, 2]) #correct input
  y_ = tf.placeholder(tf.float32, [4, 1]) #correct output
  b1 = tf.Variable(tf.zeros([2])) #bias
  b2 = tf.Variable(tf.zeros([1])) #bias 
  t1 = tf.Variable(tf.random_uniform([2,2], -1, 1)) #theta1
  t2 = tf.Variable(tf.random_uniform([2,1], -1, 1)) #theta2
  layer2 = tf.sigmoid(tf.matmul(x_, t1) + b1)
  y = tf.sigmoid(tf.matmul(layer2, t2) + b2) #training output

  x_feed = [[0,0],[0,1],[1,0],[1,1]]
  y_feed = [[0],[1],[1],[0]]

{% endhighlight %}

### gradient descent

Use a loss function : l2 norm works well and minimize it through gradient descent algorithm.

{% highlight python linenos %}
  loss = tf.nn.l2_loss(y_ - y)
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
{% endhighlight %}

### train the model
{% highlight python linenos %}
  for i in range(10000):
    sess.run(train_step, feed_dict={x_: x_feed, y_: y_feed})
{% endhighlight %}

## All the code

{% highlight python linenos %}
  from __future__ import absolute_import
  from __future__ import division
  from __future__ import print_function

  import tensorflow as tf

  def main(_):

    # Create the model
    x_ = tf.placeholder(tf.float32, [4, 2]) #correct input
    y_ = tf.placeholder(tf.float32, [4, 1]) #correct output
    b1 = tf.Variable(tf.zeros([2])) #bias
    b2 = tf.Variable(tf.zeros([1])) #bias 
    t1 = tf.Variable(tf.random_uniform([2,2], -1, 1)) #theta1
    t2 = tf.Variable(tf.random_uniform([2,1], -1, 1)) #theta2
    layer2 = tf.sigmoid(tf.matmul(x_, t1) + b1)
    y = tf.sigmoid(tf.matmul(layer2, t2) + b2) #training output

    x_feed = [[0,0],[0,1],[1,0],[1,1]]
    y_feed = [[0],[1],[1],[0]]

    loss = tf.nn.l2_loss(y_ - y)
    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

    sess = tf.InteractiveSession()
    tf.global_variables_initializer().run()
    # Train
    for i in range(10000):
      sess.run(train_step, feed_dict={x_: x_feed, y_: y_feed})
      if i % 100 == 0:
        print('y ', sess.run(y, feed_dict={x_: x_feed, y_: y_feed}))
        print('cost ', sess.run(cost, feed_dict={x_: x_feed, y_: y_feed}))

  if __name__ == '__main__':
    tf.app.run(main=main)
{% endhighlight %}    

You can fin the code on Github [here](https://github.com/exced/xor_neural_net).










